{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXPCStKPqoyK"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index langchain sentence-transformers faiss-cpu\n",
        "import os\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def load_and_process_data(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    documents = []\n",
        "    for day_dict in data:\n",
        "      for day, entries in day_dict.items():\n",
        "        for entry in entries:\n",
        "            doc = {\n",
        "                'content': entry,  # Store the full content\n",
        "                'metadata': {\n",
        "                    'day': day,\n",
        "                    'device_id': entry.get('deviceid'),\n",
        "                    'name': entry.get('name'),\n",
        "                    'sensor_type': entry.get('sensortype')\n",
        "                }\n",
        "            }\n",
        "            documents.append(doc)\n",
        "\n",
        "    return documents\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Asta/iot_data.json'\n",
        "documents = load_and_process_data(file_path)\n",
        "print (type(documents[0].get('content')))"
      ],
      "metadata": {
        "id": "WyxaCILUrznd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Document\n",
        "document_objects = []\n",
        "for doc in documents:\n",
        "    content = json.dumps(doc['content'])\n",
        "    # metadata = json.dumps(metadata)\n",
        "    document_objects.append(Document(text=content, metadata=doc['metadata']))\n"
      ],
      "metadata": {
        "id": "2WTNnbSZr0w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index langchain sentence-transformers faiss-cpu\n",
        "!pip install langchain\n",
        "!pip install llama-index-vector-stores-faiss\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "import faiss\n",
        "import json\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from llama_index.core import Document, VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from llama_index.core.storage.storage_context import StorageContext\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "d = 384\n",
        "faiss_index = faiss.IndexFlatL2(d)      # 384 is the dimension for the chosen embedding model\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(documents=document_objects, storage_context=storage_context,embed_model=embed_model)"
      ],
      "metadata": {
        "id": "H4y_rIYcr08r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_initial_info():\n",
        "    day = input(\"Enter the day (e.g., Day1, Day2): \")\n",
        "    room = input(\"Enter the room (e.g., room1, room2): \")\n",
        "    sensor_name = input(\"Enter the sensor name (e.g., FAN1, LIGHT1): \")\n",
        "    return day, room, sensor_name\n",
        "\n",
        "def get_user_query():\n",
        "    return input(\"What would you like to know about this sensor? \")"
      ],
      "metadata": {
        "id": "vHD7tBuAr1Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_documents(documents, metadata_filter):\n",
        "    filtered_docs = []\n",
        "    for doc in documents:\n",
        "        if all(doc['metadata'].get(k) == v for k, v in metadata_filter.items()):\n",
        "            filtered_docs.append(doc)\n",
        "    return filtered_docs\n",
        "\n",
        "def process_query(index, day, room, sensor_name, user_query):\n",
        "    # Construct metadata filter\n",
        "    metadata_filter = {\n",
        "        \"day\": day,\n",
        "        \"device_id\": room,\n",
        "        \"name\": sensor_name\n",
        "    }\n",
        "\n",
        "    # Filter documents based on metadata\n",
        "    filtered_docs = filter_documents(documents, metadata_filter)\n",
        "    print(f\"Filtered documents count: {len(filtered_docs)}\")\n",
        "\n",
        "    # Convert filtered documents to Document instances\n",
        "    filtered_document_objects = [Document(text=doc['content'], metadata=doc['metadata']) for doc in filtered_docs]\n",
        "    print(f\"Filtered document objects count: {len(filtered_document_objects)}\")\n",
        "\n",
        "    # Create a temporary index for the filtered documents\n",
        "    temp_index = VectorStoreIndex.from_documents(documents=filtered_document_objects, storage_context=storage_context, embed_model=embed_model)\n",
        "    print(\"Temporary index keys:\", temp_index.index_struct.nodes_dict.keys())\n",
        "    try:\n",
        "        response = temp_index.as_query_engine(llm=llm).query(user_query)\n",
        "        return response.response, response.source_nodes\n",
        "    except KeyError as e:\n",
        "        print(f\"KeyError: {e}. The key was not found in the index_struct.nodes_dict.\")\n",
        "        return \"An error occurred while processing the query.\", []\n",
        "\n",
        "    return response.response, response.source_nodes"
      ],
      "metadata": {
        "id": "i4ews4S4sELa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes accelerate einops"
      ],
      "metadata": {
        "id": "Y5bYnINcsIHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install llama-index-llms-huggingface\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "name = \"StabilityAI/stablelm-tuned-alpha-3b\"\n",
        "# Set auth token variable from hugging face\n",
        "auth_token = \"\"\n",
        "\n",
        "# Create tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(name, cache_dir='./model/', use_auth_token=auth_token)\n",
        "\n",
        "# Create model\n",
        "model = AutoModelForCausalLM.from_pretrained(name, cache_dir='./model/'\n",
        "                            , use_auth_token=auth_token, torch_dtype=torch.float16,\n",
        "                            rope_scaling={\"type\": \"dynamic\", \"factor\": 2}, load_in_8bit=True)\n",
        "\n",
        "llm = HuggingFaceLLM(context_window=4096,\n",
        "                    max_new_tokens=256,\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer\n",
        "                    model_kwargs={\"temperature\": 0.7, \"top_p\": 0.95})"
      ],
      "metadata": {
        "id": "h3EZ2xmysPW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(context, user_query):\n",
        "    prompt = f\"\"\"Based on the following context about a sensor:\n",
        "\n",
        "{context}\n",
        "\n",
        "Please answer the following question:\n",
        "{user_query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    return llm(prompt)"
      ],
      "metadata": {
        "id": "YzfUYz4AsSPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Get initial information\n",
        "    day, room, sensor_name = get_initial_info()\n",
        "\n",
        "    while True:\n",
        "        # Get user query\n",
        "        user_query = get_user_query()\n",
        "        if user_query.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        # Process query and get context\n",
        "        context, source_nodes = process_query(index, day, room, sensor_name, user_query)\n",
        "\n",
        "        # Generate answer using LLM\n",
        "        if context != \"An error occurred while processing the query.\":\n",
        "            answer = generate_answer(context, user_query)\n",
        "        else:\n",
        "            answer = context\n",
        "\n",
        "        print(f\"\\nAnswer: {answer}\")\n",
        "        print(\"\\nSources:\")\n",
        "        for node in source_nodes:\n",
        "            print(f\"- {node.metadata}\")\n",
        "        print(\"---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "QdQsI5ZesTF6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}